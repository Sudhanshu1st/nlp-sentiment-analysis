{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"\"\"\n",
    "Operator: Good day, everyone. Welcome to the Apple Incorporated Third Quarter Fiscal Year 2020 Earnings Conference Call. \n",
    "    Today's call is being recorded. At this time, for opening remarks and introductions, I would like to turn things over to Mr. Tejas Gala,\n",
    "    Senior Manager, Corporate Finance and Investor Relations. Please go ahead, sir.\n",
    "    Tejas Gala: Thank you. Good afternoon and thank you for joining us. Speaking first today is Apple's CEO, Tim Cook; and he'll be followed \n",
    "    by CFO, Luca Maestri. After that, we'll open the call to questions from analysts. Please note that some of the information you'll hear \n",
    "    during our discussion today will consist of forward-looking statements including without limitation those regarding revenue, gross margin, \n",
    "    operating expenses, other income and expense, taxes, capitalallocation, and future business outlook, including the potential impact of \n",
    "    COVID-19 on the company's business and results of operations. Actual results or trends could differ materially from our forecast. For \n",
    "    more information, please refer to the risk factors discussed in Apple's most recently filed periodic reports Form 10-K and Form 10-Q \n",
    "    and the Form 8-K filed with the SEC today along with the associated press release. Apple assumes no obligation to update any forward-looking \n",
    "    statements or information, which speak as of their respective dates. I'd now like to turn the call over to Tim for introductory remarks.\n",
    "    Tim Cook: Thanks, Tejas. Good afternoon, everyone. Thanks for joining the call today. Before we begin, I joined the many millions across \n",
    "    this country in mourning and memorialize Congressman John Lewis, who was laid to rest earlier today. We've lost a hero who walked among \n",
    "    us, a leader in the truest sense who urged this country to aim higher and be better until the very end.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [' '.join(sent.split()).strip() for sent in transcript.replace('\\n', '').split('. ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sentences, columns=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dependency in (\"brown\", \"names\", \"wordnet\", \"averaged_perceptron_tagger\", \"universal_tagset\"):\n",
    "    nltk.download(dependency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the wordnet object value corresponding to the POS tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text, digits=False, stop_words=False, lemmatize=False, only_noun=False):\n",
    "    # lower text\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    \n",
    "    # remove words that contain numbers\n",
    "    if digits:\n",
    "        text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "        \n",
    "    # remove stop words\n",
    "    if stop_words:\n",
    "        stop = stopwords.words('english')\n",
    "        text = [x for x in text if x not in stop]\n",
    "    \n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    \n",
    "    # pos tag text\n",
    "    if lemmatize:\n",
    "        pos_tags = pos_tag(text)    \n",
    "        # lemmatize text\n",
    "        text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "        \n",
    "    if only_noun:\n",
    "        # select only nouns\n",
    "        is_noun = lambda pos: pos[:2] == 'NN'\n",
    "        text = [word for (word, pos) in pos_tag(text) if is_noun(pos)]\n",
    "    \n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    \n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# clean text data\n",
    "df['content_clean'] = df['content'].apply(lambda x: clean_text(x, digits=True, stop_words=True, lemmatize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sentiment anaylsis columns\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df['sentiment']= df['content_clean'].apply(lambda x: sid.polarity_scores(x))\n",
    "df = pd.concat([df.drop(['sentiment'], axis=1), df['sentiment'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'neu': 'neutral', 'neg': 'negative', 'pos': 'positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['confidence'] = df[[\"negative\", \"neutral\", \"positive\"]].max(axis=1)\n",
    "df['sentiment'] = df[[\"negative\", \"neutral\", \"positive\"]].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = pd.DataFrame(df['sentiment'].value_counts()).reset_index()\n",
    "grouped.columns = ['sentiment','count']\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display percentage of positive, negative and neutral sentiments\n",
    "fig = px.pie(grouped, values='count', names='sentiment', title='Sentiments')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_ratio = df['sentiment'].value_counts(normalize=True).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['negative', 'neutral', 'positive']:\n",
    "    if key not in sentiment_ratio:\n",
    "        sentiment_ratio[key] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display sentiment score\n",
    "\n",
    "sentiment_score = (sentiment_ratio['neutral'] + sentiment_ratio['positive']) - sentiment_ratio['negative']\n",
    "\n",
    "fig = go.Figure(go.Indicator(\n",
    "    mode = \"number+delta\",\n",
    "    value = sentiment_score,\n",
    "    delta = {\"reference\": 0.5},\n",
    "    title = {\"text\": \"Sentiment Score\"},))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display negative sentence locations\n",
    "fig = px.scatter(df, y='sentiment', color='sentiment', size='confidence', hover_data=['content'], color_discrete_map={\"negative\":\"firebrick\",\"neutral\":\"navajowhite\",\"positive\":\"darkgreen\"})\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disply annotated trasncript\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate (record):\n",
    "    line = f\"\"\"<span class=\"highlight {record['sentiment']}\">{record['content']} </span>\"\"\"\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for record in df[['content', 'sentiment']].to_dict('records'):\n",
    "    text += annotate(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
